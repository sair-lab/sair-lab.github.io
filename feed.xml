<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://sairlab.org/feed.xml" rel="self" type="application/atom+xml" /><link href="https://sairlab.org/" rel="alternate" type="text/html" /><updated>2022-07-05T17:49:24+00:00</updated><id>https://sairlab.org/feed.xml</id><title type="html">Chen Wang</title><subtitle>Chen Wang is a Project Scientist with the Robotics Institute at Carnegie Mellon University.</subtitle><entry><title type="html">AirObject: A Temporally Evolving Graph Embedding for Object Identification</title><link href="https://sairlab.org/airobject/" rel="alternate" type="text/html" title="AirObject: A Temporally Evolving Graph Embedding for Object Identification" /><published>2022-03-15T12:00:00+00:00</published><updated>2022-03-15T12:00:00+00:00</updated><id>https://sairlab.org/airobject</id><content type="html" xml:base="https://sairlab.org/airobject/">&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-03-15-airobject/obj_1_org.gif&quot; /&gt;
    &lt;figcaption&gt;
       Video Object Identification
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Object encoding and identification are vital for robotic tasks such as autonomous exploration, semantic scene understanding, and re-localization. Previous approaches have attempted to either track objects or generate descriptors for object identification. However, such systems are limited to a “fixed” object representation from a single viewpoint and are not robust to severe occlusion, viewpoint shift, perceptual aliasing, or scale transform. These single frame representations tend to lead to false correspondences amongst perceptually-aliased objects, especially when severely occluded. Hence, we propose one of the first temporal object encoding methods, &lt;strong&gt;AirObject&lt;/strong&gt;, that aggregates the temporally “evolving” object structure as the camera or object moves. The AirObject descriptors, which accumulate knowledge across multiple evolving representations of the objects, are robust to severe occlusion, viewpoint changes, deformation, perceptual aliasing, and the scale transform.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-03-15-airobject/overview.jpg&quot; /&gt;
    &lt;figcaption&gt;
        Matching Temporally Evolving Representations using AirObject
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;topological-object-representations&quot;&gt;Topological Object Representations&lt;/h2&gt;

&lt;p&gt;Intuitively, a group of feature points on an object form a graphical representation where the feature points are nodes and their associated descriptors are the node features. Essentially, the graph’s nodes are the distinctive local features of the object, while the edges/structure of the graph represents the global structure of the object. Hence, to learn the geometric relationship of the feature points, we construct topological object graphs for each frame using Delaunay triangulation. The obtained triangular mesh representation enables the graph attention encoder to reason better about the object’s structure, thereby making the final temporal object descriptor robust to deformation or occlusion.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-03-15-airobject/triangulation.png&quot; /&gt;
    &lt;figcaption&gt;
        Topological Graph Representations of Objects. These representations are generated by using Delaunay Triangulation on object-wise grouped SuperPoint keypoints.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;simple--effective-temporal-object-encoding-method&quot;&gt;Simple &amp;amp; Effective Temporal Object Encoding Method&lt;/h2&gt;

&lt;p&gt;Our proposed method is very simple and only contains three modules. Specifically, we use extracted deep learned keypoint features across multiple frames to form sequences of object-wise topological graph neural networks (GNNs), which on embedding generate temporal object descriptors. We employ a graph attention-based sparse encoding method on these topological GNNs to generate content graph features and location graph features representing the structural information of the object. Then, these graph features are aggregated across multiple frames using a single-layer temporal convolutional network to generate a temporal object descriptor. These generated object descriptors are robust to severe occlusion, perceptual aliasing, viewpoint shift, deformation, and scale transform, outperforming the state-of-the-art single-frame and sequential descriptors.&lt;/p&gt;

&lt;figure&gt;
    &lt;figcaption&gt;
       Video Object Identification
    &lt;/figcaption&gt;
    &lt;img src=&quot;/img/posts/2022-03-15-airobject/obj_2.gif&quot; /&gt;
    &lt;img src=&quot;/img/posts/2022-03-15-airobject/obj_3.gif&quot; /&gt;
    &lt;img src=&quot;/img/posts/2022-03-15-airobject/obj_4.gif&quot; /&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;video&quot;&gt;Video&lt;/h2&gt;

&lt;div class=&quot;video-wrapper&quot;&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/lTEXcKW_aWg&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;h2 id=&quot;publication&quot;&gt;Publication&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@inproceedings{keetha2022airobject,
 title={AirObject: A Temporally Evolving Graph Embedding for Object Identification},
 author={Keetha, Nikhil Varma and Wang, Chen and Qiu, Yuheng and Xu, Kuan and Scherer, Sebastian}, 
 booktitle={2022 Conference on Computer Vision and Pattern Recognition (CVPR)},
 year={2022},
 url={https://arxiv.org/abs/2111.15150}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Please refer to our &lt;a href=&quot;https://arxiv.org/abs/2111.15150&quot;&gt;Paper&lt;/a&gt; and &lt;a href=&quot;https://github.com/Nik-V9/AirObject&quot;&gt;Code&lt;/a&gt; for details.&lt;/p&gt;

&lt;h2 id=&quot;contact&quot;&gt;Contact&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://nik-v9.github.io/&quot;&gt;Nikhil Varma Keetha&lt;/a&gt; &amp;lt;keethanikhil [at] gmail [dot] com&amp;gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://chenwang.site&quot;&gt;Chen Wang&lt;/a&gt; &amp;lt;chenwang [at] dr.com&amp;gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://theairlab.org/team/sebastian/&quot;&gt;Sebastian Scherer&lt;/a&gt; &amp;lt;basti [at] cmu [dot] edu&amp;gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Nikhil Varma Keetha</name></author><category term="research" /><summary type="html">Video Object Identification</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://sairlab.org/img/posts/2022-03-15-airobject/obj_1_org.gif" /><media:content medium="image" url="https://sairlab.org/img/posts/2022-03-15-airobject/obj_1_org.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Lifelong Graph Learning</title><link href="https://sairlab.org/lgl/" rel="alternate" type="text/html" title="Lifelong Graph Learning" /><published>2022-03-05T12:00:00+00:00</published><updated>2022-03-05T12:00:00+00:00</updated><id>https://sairlab.org/lgl</id><content type="html" xml:base="https://sairlab.org/lgl/">&lt;p&gt;Graph neural networks (GNNs) are powerful models for many graph-structured tasks. Existing models often assume that a complete structure of a graph is available during training. However, graph-structured data is often formed in a streaming fashion so that learning a graph continuously is often necessary.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-03-05-lgl/growing-graph.jpg&quot; /&gt;
    &lt;figcaption&gt;
        A temporally growing graph, which is challenging to learn the graph in a sequential way.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Existing lifelong learning techniques are mostly designed for convolutional neural networks (CNNs), which &lt;em&gt;assumes the new data samples are independent&lt;/em&gt;. However, in lifelong graph learning, &lt;em&gt;nodes are connected and dynamically added&lt;/em&gt;.
In this work, we have an important observation:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;The number of nodes increases dynamically, while the number of node features is stable.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Therefore, we introduce feature graph, which &lt;strong&gt;takes features as nodes and turns nodes into independent graphs&lt;/strong&gt;. This successfully converts the original problem of node classification to graph classification, in which the increasing nodes are turned into independent training samples.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-03-05-lgl/feature-graph.jpg&quot; /&gt;
    &lt;figcaption&gt;
        Feature graph takes the features as nodes and turns nodes into graphs, resulting in a graph predictor instead of the node predictor. Take the node a with label &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=\mathbf{z}_a&quot; /&gt; in the regular graph &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=\mathcal{G}&quot; /&gt; as an example, its features &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=x_a = [1, 0, 0, 1]&quot; /&gt;  are nodes &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=\{a1, a2, a3, a4\}&quot; /&gt; in feature graph &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=\mathcal{G}^F&quot; /&gt;. 
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The feature adjacency is established via feature cross-correlation between &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=a&quot; /&gt; and its neighbors &lt;img src=&quot;https://render.githubusercontent.com/render/math?math=\mathcal{N}(a) = \{a, b, c, d, e\}&quot; /&gt; to model feature “interaction.”  This makes the lifelong learning techniques for CNN applicable to GNN, as the new nodes in a regular graph become individual training samples.&lt;/p&gt;

&lt;figure&gt;
    &lt;figcaption&gt;
        The relationship of a graph and feature graph.
    &lt;/figcaption&gt;
    &lt;img src=&quot;/img/posts/2022-03-05-lgl/relationship.jpg&quot; /&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;applications&quot;&gt;Applications&lt;/h2&gt;

&lt;h4 id=&quot;1-feature-matching&quot;&gt;1. Feature Matching&lt;/h4&gt;

&lt;p&gt;Image feature matching is crucial for many 3-D computer vision tasks including simultaneous localization and mapping (SLAM).
As shown below, the interest point and their descriptors form an infinite temporal growing graph, in which the feature points are nodes and their descriptors are the node features. In this way, the problem of feature matching becomes edge prediction for a temporal growing graph.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-03-05-lgl/matching.jpg&quot; /&gt;
    &lt;figcaption&gt;
         Feature matching is a problem of edge prediction for temporal growing graph.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In an environment with dynamic illumination, feature graph network can also achieve much better accuracy and robustness, which demonstrate its effectiveness.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-03-05-lgl/matching.gif&quot; /&gt;
    &lt;figcaption&gt;
        Feature fatching in an environment with dynamic illumination.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;2-distributed-human-action-recognition-with-wearable-devices&quot;&gt;2. Distributed Human Action Recognition with Wearable Devices&lt;/h4&gt;

&lt;p&gt;Five sensors, each of which consists of a triaxial accelerometer and a biaxial gyroscope, are located at the left and right forearms, waist, left and right ankles, respectively. Each sensor produces 5 data streams and totally 5 × 5 data streams is available. 13 daily action categories are considered, including rest at standing (ReSt), rest at sitting (ReSi), rest at lying (ReLi), walk forward (WaFo), walk forward left-circle (WaLe), walk forward right-circle (WaRi), turn left (TuLe), turn right (TuRi), go upstairs (Up), go downstairs (Down), jog (Jog), jump (Jump), and push wheelchair (Push). Therefore, action recognition is a problem of sub-graph classification.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-03-05-lgl/ward.jpg&quot; /&gt;
    &lt;figcaption&gt;
        Lifelong human action recognition is a problem of lifelong sub-graph classification.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Our feature graph network (FGN) has a much higher and stable performance than all the other
methods, including GCN, APPNP, and GAT. It also achieves a much higher final per-class precision in nearly all the categories.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;FGN is the first method to bridge graph learning to lifelong learning via a novel graph topology.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;publication&quot;&gt;Publication&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Chen Wang, Yuheng Qiu, Dasong Gao, Sebastian Scherer. &lt;a href=&quot;https://arxiv.org/pdf/2009.00647.pdf&quot;&gt;Lifelong Graph Learning&lt;/a&gt;. “&lt;em&gt;2022 Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/em&gt;, 2022.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://arxiv.org/pdf/2009.00647.pdf&quot;&gt;Download PDF&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;source-code&quot;&gt;Source Code&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/wang-chen/LGL&quot;&gt;Lifelong Graph Learning (Citation Graph)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/wang-chen/lgl-action-recognition&quot;&gt;Lifelong Graph Learning (Action Recognition)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://github.com/wang-chen/lgl-feature-matching&quot;&gt;Lifelong Graph Learning (Feature Matching)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;citation&quot;&gt;Citation&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@inproceedings{wang2022lifelong,
 title={Lifelong graph learning},
 author={Wang, Chen and Gao, Dasong and Qiu, Yuheng and Scherer, Sebastian},
 booktitle={2022 Conference on Computer Vision and Pattern Recognition (CVPR)},
 year={2022}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;contact&quot;&gt;Contact&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://chenwang.site&quot;&gt;Chen Wang&lt;/a&gt; &amp;lt;chenwang [at] dr.com&amp;gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Chen Wang</name></author><category term="research" /><summary type="html">Graph neural networks (GNNs) are powerful models for many graph-structured tasks. Existing models often assume that a complete structure of a graph is available during training. However, graph-structured data is often formed in a streaming fashion so that learning a graph continuously is often necessary.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://sairlab.org/img/posts/2022-03-05-lgl/matching.jpg" /><media:content medium="image" url="https://sairlab.org/img/posts/2022-03-05-lgl/matching.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">AirDOS: Dynamic SLAM benefits from Articulated Objects</title><link href="https://sairlab.org/airdos" rel="alternate" type="text/html" title="AirDOS: Dynamic SLAM benefits from Articulated Objects" /><published>2022-02-06T12:00:00+00:00</published><updated>2022-02-06T12:00:00+00:00</updated><id>https://sairlab.org/airdos</id><content type="html" xml:base="https://sairlab.org/airdos">&lt;p&gt;Dynamic Object-aware SLAM (DOS) exploits object-level information to enable robust motion estimation in
dynamic environments. It has attracted increasing attention with the recent success of learning-based models. Existing methods mainly focus on identifying and excluding dynamic objects from the optimization. In this work, we show that feature-based visual SLAM systems can also benefit from the presence of dynamic articulated objects by taking advantage of two observations:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The 3D structure of an articulated object remains consistent over time;&lt;/li&gt;
  &lt;li&gt;The points on the same object follow the same motion.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-02-06-airdos/airdos.gif&quot; /&gt;
    &lt;figcaption&gt;
        AirDOS demo in TartanAir Shibuya dataset
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In particular, we present &lt;strong&gt;AirDOS&lt;/strong&gt;, a dynamic object-aware system that introduces rigidity and motion constraints to model articulated objects. By jointly optimizing the camera pose, object motion, and the object 3D
structure, we can rectify the camera pose estimation, preventing tracking lost, and generate 4D spatio-temporal maps for both dynamic objects and static scenes.&lt;/p&gt;

&lt;h2 id=&quot;articulated-objects&quot;&gt;Articulated Objects&lt;/h2&gt;

&lt;p&gt;We extend the simple rigid objects to general articulated objects, which are defined as
objects composed of one or more rigid parts connected by joints allowing rotational or
translational motion, e.g., vehicles and humans, and utilize the properties of
articulated objects to improve the camera pose estimation.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2022-02-06-airdos/airticulated.jpg&quot; /&gt;
    &lt;figcaption&gt;
        An example of the articulated dynamic objects’ point-segment model. Vehicles and semi-rigid objects like pedestrian are modeled as articulated objects.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;We jointly optimize the 3D structural information and the motion of articulated objects.
To this end, we introduce&lt;/p&gt;

&lt;p&gt;(1) a &lt;strong&gt;rigidity constraint&lt;/strong&gt;, which assumes that the distance between any two points located on the same rigid part remains constant over time, and&lt;/p&gt;

&lt;p&gt;(2) a &lt;strong&gt;motion constraint&lt;/strong&gt;, which assumes that feature points on the same rigid parts follow the same 3D motion. This allows us to build a 4D spatio-temporal map including both dynamic and static structures.&lt;/p&gt;

&lt;div class=&quot;video-wrapper&quot;&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/gM5iUL1B6R0&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;publication&quot;&gt;Publication&lt;/h2&gt;

&lt;p&gt;Qiu, Yuheng, Chen Wang, Wenshan Wang, Mina Henein, and Sebastian Scherer. “&lt;a href=&quot;https://arxiv.org/abs/2109.09903&quot;&gt;AirDOS: Dynamic SLAM benefits from Articulated Objects&lt;/a&gt;.” International Conference on Robotics and Automation (ICRA), 2022.&lt;/p&gt;

&lt;h2 id=&quot;source-code&quot;&gt;Source Code&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/haleqiu/AirDOS&quot;&gt;https://github.com/haleqiu/AirDOS&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;citation&quot;&gt;Citation&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@inproceedings{qiu2022airdos,
 title={AirDOS: Dynamic SLAM benefits from Articulated Objects},
 author={Qiu, Yuheng and Wang, Chen and Wang, Wenshan and Henein, Mina and Scherer, Sebastian},
 booktitle={International Conference on Robotics and Automation (ICRA)},
 year={2022}
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;contact&quot;&gt;Contact&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://theairlab.org/team/yuheng/&quot;&gt;Yuheng Qiu&lt;/a&gt; &amp;lt;yuhengq [at] andrew [dot] cmu [dot] edu&amp;gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://chenwang.site&quot;&gt;Chen Wang&lt;/a&gt; &amp;lt;chenwang [at] dr.com&amp;gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.wangwenshan.com/&quot;&gt;Wenshan Wang&lt;/a&gt; &amp;lt;wenshanw [at] andrew [dot] cmu [dot] edu&amp;gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://theairlab.org/team/sebastian/&quot;&gt;Sebastian Scherer&lt;/a&gt; &amp;lt;basti [at] cmu [dot] edu&amp;gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Yuheng Qiu</name></author><category term="research" /><summary type="html">Dynamic Object-aware SLAM (DOS) exploits object-level information to enable robust motion estimation in dynamic environments. It has attracted increasing attention with the recent success of learning-based models. Existing methods mainly focus on identifying and excluding dynamic objects from the optimization. In this work, we show that feature-based visual SLAM systems can also benefit from the presence of dynamic articulated objects by taking advantage of two observations:</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://sairlab.org/img/posts/2022-02-06-airdos/AirDOS-title.gif" /><media:content medium="image" url="https://sairlab.org/img/posts/2022-02-06-airdos/AirDOS-title.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Air Series Articles Released</title><link href="https://sairlab.org/airseries/" rel="alternate" type="text/html" title="Air Series Articles Released" /><published>2021-12-31T00:00:00+00:00</published><updated>2021-12-31T00:00:00+00:00</updated><id>https://sairlab.org/air-series</id><content type="html" xml:base="https://sairlab.org/airseries/">&lt;p&gt;&lt;strong&gt;Air Series&lt;/strong&gt; is a collection of &lt;strong&gt;articles mentored by Chen Wang&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;A wide variety of topics in &lt;strong&gt;robotics&lt;/strong&gt; are covered, including &lt;strong&gt;localization&lt;/strong&gt;, &lt;strong&gt;detection&lt;/strong&gt;, and &lt;strong&gt;lifelong learning&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;All articles are &lt;strong&gt;first authored by Undergraduate or Master students&lt;/strong&gt; and &lt;strong&gt;second authored by Chen Wang&lt;/strong&gt;.&lt;/p&gt;

&lt;style&gt;
.csl-block {
    font-size: 16px;
}
.csl-title, .csl-author, .csl-event, .csl-editor, .csl-venue {
    display: block;
    position: relative;
    font-size: 15px;
}

.csl-title b {
    font-weight: 600;
}

.csl-content {
    display: inline-block;
    vertical-align: top;
    padding-left: 20px;
}

.bibliography {
   list-style-type: none;
}
&lt;/style&gt;

&lt;h2 id=&quot;air-series-articles&quot;&gt;Air Series Articles&lt;/h2&gt;

&lt;ul.no-bullet class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;
    &lt;span id=&quot;li2021airdet&quot;&gt;[1]&lt;div class=&quot;csl-block csl-content&quot;&gt;&lt;div class=&quot;csl-block csl-title&quot;&gt;&lt;b&gt;AirDet: Few-Shot Detection without Fine-tuning for Autonomous Exploration&lt;/b&gt;.&lt;/div&gt;&lt;div class=&quot;csl-block csl-author&quot;&gt;Bowen Li, &lt;b&gt;Chen&lt;/b&gt; &lt;b&gt;Wang&lt;/b&gt;, Pranay Reddy, Seungchan Kim, Sebastian Scherer.&lt;/div&gt;&lt;div class=&quot;csl-block csl-event&quot;&gt;&lt;i&gt;European Conference on Computer Vision (ECCV)&lt;/i&gt;, 2022.&lt;/div&gt;&lt;/div&gt;&lt;/span&gt;
    &lt;br /&gt;
&lt;button class=&quot;button0&quot; onclick=&quot;toggleli2021airdet()&quot;&gt;bibtex&lt;/button&gt;

&lt;script&gt;
    function toggleli2021airdet() {
        var x= document.getElementById('ali2021airdet');
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
&lt;/script&gt;



&lt;script&gt;
    function toggle2li2021airdet() {
        var x= document.getElementById('bli2021airdet');
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
&lt;/script&gt;





&lt;a href=&quot;https://arxiv.org/pdf/2112.01740&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button4&quot; value=&quot;PDF&quot; /&gt;&lt;/a&gt;





&lt;/div&gt;

&lt;div id=&quot;ali2021airdet&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;@inproceedings{li2021airdet,
  title = {AirDet: Few-Shot Detection without Fine-tuning for Autonomous Exploration},
  author = {Li, Bowen and &lt;b&gt;Wang&lt;/b&gt;, &lt;b&gt;Chen&lt;/b&gt; and Reddy, Pranay and Kim, Seungchan and Scherer, Sebastian},
  booktitle = {European Conference on Computer Vision (ECCV)},
  year = {2022},
  url = {https://arxiv.org/pdf/2112.01740},
  keywords = {airseries}
}
&lt;/pre&gt;&lt;/div&gt;
&lt;!-- &lt;div id=&quot;bli2021airdet&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;&lt;/pre&gt;&lt;/div&gt; --&gt;
&lt;/li&gt;
&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;
    &lt;span id=&quot;keetha2021airobject&quot;&gt;[2]&lt;div class=&quot;csl-block csl-content&quot;&gt;&lt;div class=&quot;csl-block csl-title&quot;&gt;&lt;b&gt;AirObject: A Temporally Evolving Graph Embedding for Object Identification&lt;/b&gt;.&lt;/div&gt;&lt;div class=&quot;csl-block csl-author&quot;&gt;Nikhil Varma Keetha, &lt;b&gt;Chen&lt;/b&gt; &lt;b&gt;Wang&lt;/b&gt;, Yuheng Qiu, Kuan Xu, Sebastian Scherer.&lt;/div&gt;&lt;div class=&quot;csl-block csl-event&quot;&gt;&lt;i&gt;IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/i&gt;, 2022.&lt;/div&gt;&lt;/div&gt;&lt;/span&gt;
    &lt;br /&gt;
&lt;button class=&quot;button0&quot; onclick=&quot;togglekeetha2021airobject()&quot;&gt;bibtex&lt;/button&gt;

&lt;script&gt;
    function togglekeetha2021airobject() {
        var x= document.getElementById('akeetha2021airobject');
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
&lt;/script&gt;



&lt;script&gt;
    function toggle2keetha2021airobject() {
        var x= document.getElementById('bkeetha2021airobject');
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
&lt;/script&gt;





&lt;a href=&quot;https://arxiv.org/pdf/2111.15150&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button4&quot; value=&quot;PDF&quot; /&gt;&lt;/a&gt;


&lt;a href=&quot;https://github.com/Nik-V9/AirObject&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button3&quot; value=&quot;Code&quot; /&gt;&lt;/a&gt;


&lt;a href=&quot;https://youtu.be/lTEXcKW_aWg&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button3&quot; value=&quot;Video&quot; /&gt;&lt;/a&gt;



&lt;/div&gt;

&lt;div id=&quot;akeetha2021airobject&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;@inproceedings{keetha2021airobject,
  title = {AirObject: A Temporally Evolving Graph Embedding for Object Identification},
  author = {Keetha, Nikhil Varma and &lt;b&gt;Wang&lt;/b&gt;, &lt;b&gt;Chen&lt;/b&gt; and Qiu, Yuheng and Xu, Kuan and Scherer, Sebastian},
  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2022},
  url = {https://arxiv.org/pdf/2111.15150},
  video = {https://youtu.be/lTEXcKW_aWg},
  code = {https://github.com/Nik-V9/AirObject},
  keywords = {airseries}
}
&lt;/pre&gt;&lt;/div&gt;
&lt;!-- &lt;div id=&quot;bkeetha2021airobject&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;&lt;/pre&gt;&lt;/div&gt; --&gt;
&lt;/li&gt;
&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;
    &lt;span id=&quot;gao2021lifelong&quot;&gt;[3]&lt;div class=&quot;csl-block csl-content&quot;&gt;&lt;div class=&quot;csl-block csl-title&quot;&gt;&lt;b&gt;AirLoop: Lifelong Loop Closure Detection&lt;/b&gt;.&lt;/div&gt;&lt;div class=&quot;csl-block csl-author&quot;&gt;Dasong Gao, &lt;b&gt;Chen&lt;/b&gt; &lt;b&gt;Wang&lt;/b&gt;, Sebastian Scherer.&lt;/div&gt;&lt;div class=&quot;csl-block csl-event&quot;&gt;&lt;i&gt;International Conference on Robotics and Automation (ICRA)&lt;/i&gt;, 2022.&lt;/div&gt;&lt;/div&gt;&lt;/span&gt;
    &lt;br /&gt;
&lt;button class=&quot;button0&quot; onclick=&quot;togglegao2021lifelong()&quot;&gt;bibtex&lt;/button&gt;

&lt;script&gt;
    function togglegao2021lifelong() {
        var x= document.getElementById('agao2021lifelong');
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
&lt;/script&gt;



&lt;script&gt;
    function toggle2gao2021lifelong() {
        var x= document.getElementById('bgao2021lifelong');
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
&lt;/script&gt;





&lt;a href=&quot;https://arxiv.org/pdf/2109.08975&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button4&quot; value=&quot;PDF&quot; /&gt;&lt;/a&gt;


&lt;a href=&quot;https://github.com/wang-chen/AirLoop&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button3&quot; value=&quot;Code&quot; /&gt;&lt;/a&gt;


&lt;a href=&quot;https://youtu.be/Gr9i5ONNmz0&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button3&quot; value=&quot;Video&quot; /&gt;&lt;/a&gt;



&lt;/div&gt;

&lt;div id=&quot;agao2021lifelong&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;@inproceedings{gao2021lifelong,
  title = {AirLoop: Lifelong Loop Closure Detection},
  author = {Gao, Dasong and &lt;b&gt;Wang&lt;/b&gt;, &lt;b&gt;Chen&lt;/b&gt; and Scherer, Sebastian},
  booktitle = {International Conference on Robotics and Automation (ICRA)},
  year = {2022},
  url = {https://arxiv.org/pdf/2109.08975},
  code = {https://github.com/wang-chen/AirLoop},
  video = {https://youtu.be/Gr9i5ONNmz0},
  keywords = {airseries}
}
&lt;/pre&gt;&lt;/div&gt;
&lt;!-- &lt;div id=&quot;bgao2021lifelong&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;&lt;/pre&gt;&lt;/div&gt; --&gt;
&lt;/li&gt;
&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;
    &lt;span id=&quot;qiu2021airdos&quot;&gt;[4]&lt;div class=&quot;csl-block csl-content&quot;&gt;&lt;div class=&quot;csl-block csl-title&quot;&gt;&lt;b&gt;AirDOS: Dynamic SLAM benefits from Articulated Objects&lt;/b&gt;.&lt;/div&gt;&lt;div class=&quot;csl-block csl-author&quot;&gt;Yuheng Qiu, &lt;b&gt;Chen&lt;/b&gt; &lt;b&gt;Wang&lt;/b&gt;, Wenshan Wang, Mina Henein, Sebastian Scherer.&lt;/div&gt;&lt;div class=&quot;csl-block csl-event&quot;&gt;&lt;i&gt;International Conference on Robotics and Automation (ICRA)&lt;/i&gt;, 2022.&lt;/div&gt;&lt;/div&gt;&lt;/span&gt;
    &lt;br /&gt;
&lt;button class=&quot;button0&quot; onclick=&quot;toggleqiu2021airdos()&quot;&gt;bibtex&lt;/button&gt;

&lt;script&gt;
    function toggleqiu2021airdos() {
        var x= document.getElementById('aqiu2021airdos');
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
&lt;/script&gt;



&lt;script&gt;
    function toggle2qiu2021airdos() {
        var x= document.getElementById('bqiu2021airdos');
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
&lt;/script&gt;





&lt;a href=&quot;https://arxiv.org/pdf/2109.09903&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button4&quot; value=&quot;PDF&quot; /&gt;&lt;/a&gt;


&lt;a href=&quot;https://github.com/haleqiu/airdos&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button3&quot; value=&quot;Code&quot; /&gt;&lt;/a&gt;


&lt;a href=&quot;https://youtu.be/gM5iUL1B6R0&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button3&quot; value=&quot;Video&quot; /&gt;&lt;/a&gt;



&lt;/div&gt;

&lt;div id=&quot;aqiu2021airdos&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;@inproceedings{qiu2021airdos,
  title = {AirDOS: Dynamic SLAM benefits from Articulated Objects},
  author = {Qiu, Yuheng and &lt;b&gt;Wang&lt;/b&gt;, &lt;b&gt;Chen&lt;/b&gt; and Wang, Wenshan and Henein, Mina and Scherer, Sebastian},
  booktitle = {International Conference on Robotics and Automation (ICRA)},
  year = {2022},
  url = {https://arxiv.org/pdf/2109.09903},
  code = {https://github.com/haleqiu/airdos},
  video = {https://youtu.be/gM5iUL1B6R0},
  keywords = {airseries}
}
&lt;/pre&gt;&lt;/div&gt;
&lt;!-- &lt;div id=&quot;bqiu2021airdos&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;&lt;/pre&gt;&lt;/div&gt; --&gt;
&lt;/li&gt;
&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;
    &lt;span id=&quot;xu2021aircode&quot;&gt;[5]&lt;div class=&quot;csl-block csl-content&quot;&gt;&lt;div class=&quot;csl-block csl-title&quot;&gt;&lt;b&gt;AirCode: A Robust Object Encoding Method&lt;/b&gt;. &lt;/div&gt;&lt;div class=&quot;csl-block csl-author&quot;&gt;Kuan Xu, &lt;b&gt;Chen&lt;/b&gt; &lt;b&gt;Wang&lt;/b&gt;, Chao Chen, Wei Wu, Sebastian Scherer.&lt;/div&gt;&lt;div class=&quot;csl-block csl-event&quot;&gt;&lt;i&gt;IEEE Robotics and Automation Letters (RA-L)&lt;/i&gt;, 2022.&lt;/div&gt;&lt;/div&gt;&lt;/span&gt;
    &lt;br /&gt;
&lt;button class=&quot;button0&quot; onclick=&quot;togglexu2021aircode()&quot;&gt;bibtex&lt;/button&gt;

&lt;script&gt;
    function togglexu2021aircode() {
        var x= document.getElementById('axu2021aircode');
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
&lt;/script&gt;



&lt;script&gt;
    function toggle2xu2021aircode() {
        var x= document.getElementById('bxu2021aircode');
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
&lt;/script&gt;





&lt;a href=&quot;https://arxiv.org/pdf/2105.00327&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button4&quot; value=&quot;PDF&quot; /&gt;&lt;/a&gt;


&lt;a href=&quot;https://github.com/wang-chen/AirCode&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button3&quot; value=&quot;Code&quot; /&gt;&lt;/a&gt;


&lt;a href=&quot;https://youtu.be/ZhW4Qk1tLNQ&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button3&quot; value=&quot;Video&quot; /&gt;&lt;/a&gt;


&lt;b style=&quot;color:gray;&quot;&gt;Accepted to ICRA 2022&lt;/b&gt;


&lt;/div&gt;

&lt;div id=&quot;axu2021aircode&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;@article{xu2021aircode,
  title = {AirCode: A Robust Object Encoding Method},
  author = {Xu, Kuan and &lt;b&gt;Wang&lt;/b&gt;, &lt;b&gt;Chen&lt;/b&gt; and Chen, Chao and Wu, Wei and Scherer, Sebastian},
  journal = {IEEE Robotics and Automation Letters (RA-L)},
  year = {2022},
  url = {https://arxiv.org/pdf/2105.00327},
  code = {https://github.com/wang-chen/AirCode},
  video = {https://youtu.be/ZhW4Qk1tLNQ},
  keywords = {airseries},
  addinfo = {Accepted to ICRA 2022}
}
&lt;/pre&gt;&lt;/div&gt;
&lt;!-- &lt;div id=&quot;bxu2021aircode&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;&lt;/pre&gt;&lt;/div&gt; --&gt;
&lt;/li&gt;&lt;/ul.no-bullet&gt;

&lt;h3 id=&quot;first-author-information-when-work-was-done&quot;&gt;First Author Information (When work was done)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://jaraxxus-me.github.io/&quot;&gt;Bowen Li&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;RISS intern at Carnegie Mellon University.&lt;/li&gt;
      &lt;li&gt;Junior student at Tongji University, China.&lt;/li&gt;
      &lt;li&gt;Now: Incoming PhD student of &lt;a href=&quot;https://www.ri.cmu.edu/&quot;&gt;CMU RI&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/in/nikhil-varma-keetha-612685193/&quot;&gt;Nikhil Varma Keetha&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;RISS intern at Carnegie Mellon University.&lt;/li&gt;
      &lt;li&gt;Junior student at Indian Institute of Technology Dhanbad.&lt;/li&gt;
      &lt;li&gt;Now: Incoming Master student of &lt;a href=&quot;https://www.ri.cmu.edu/&quot;&gt;CMU RI&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://scholar.google.com/citations?user=_loctXsAAAAJ&amp;amp;hl=en&quot;&gt;Dasong Gao&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Master student at Carnegie Mellon University.&lt;/li&gt;
      &lt;li&gt;Now: Incoming PhD student of &lt;a href=&quot;https://www.eecs.mit.edu/&quot;&gt;MIT EECS&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://scholar.google.com/citations?user=aEK45mEAAAAJ&quot;&gt;Yuheng Qiu&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Undergraduate of Chinese University of Hong Kong.&lt;/li&gt;
      &lt;li&gt;Now: PhD student of &lt;a href=&quot;https://www.meche.engineering.cmu.edu/&quot;&gt;CMU ME&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Kuan Xu
    &lt;ul&gt;
      &lt;li&gt;Master Graduate of Harbin Institute of Technology, China.&lt;/li&gt;
      &lt;li&gt;Engineer at Tencent and Geekplus.&lt;/li&gt;
      &lt;li&gt;Now: Incoming PhD student of &lt;a href=&quot;https://www.ntu.edu.sg/eee&quot;&gt;NTU EEE&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;contribution&quot;&gt;Contribution&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;AirDet: Few-shot Detection without Fine-tunning&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;The first practical few-shot object detection method that requires no fine-tunning.&lt;/li&gt;
      &lt;li&gt;It achieves even better results than the exhaustively fine-tuned methods (up to 60% improvements).&lt;/li&gt;
      &lt;li&gt;Validated on real world sequences from DARPA Subterranean (SubT) challenge.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2021-12-31-air-series/AirDet.gif&quot; /&gt;
    &lt;figcaption&gt;
        Only three examples are given for novel object detection without fine-tunning.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;AirObject: Temporal Object Embedding&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;The first temporal object embedding method.&lt;/li&gt;
      &lt;li&gt;It achieves the state-of-the-art performance for video object identification.&lt;/li&gt;
      &lt;li&gt;Robust to severe occlusion, perceptual aliasing, viewpoint shift, deformation, and scale transform.&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;/airobject&quot;&gt;Project Page: https://chenwang.site/airobject&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2021-12-31-air-series/AirObject.gif&quot; /&gt;
    &lt;figcaption&gt;
        Temporal object matching on videos.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;AirDOS: Dynamic Object-aware SLAM (DOS) system&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;The first DOS system showing that camera pose estimation can be improved by incorporating dynamic articulated objects.&lt;/li&gt;
      &lt;li&gt;Establish 4-D dynamic object maps.&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;/airdos&quot;&gt;Project Page: https://chenwang.site/airdos&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2021-12-31-air-series/AirDOS.gif&quot; /&gt;
    &lt;figcaption&gt;
        Dynamic Objects can correct the camera pose estimation.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;AirLoop: Lifelong Learning for Robots&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;The first lifelong learning method for loop closure detection.&lt;/li&gt;
      &lt;li&gt;Model incremental improvement even after deployment.&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;/airloop&quot;&gt;Project Page: https://chenwang.site/airloop&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2021-09-28-airloop/tartanair-ll.gif&quot; /&gt;
    &lt;figcaption&gt;
        The model is able to correct previously made mistakes after learning more environments.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;AirCode: Robust Object Encoding&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;The first deep point-based object encoding for single image.&lt;/li&gt;
      &lt;li&gt;It achieves the state-of-the-art performance for object re-identification.&lt;/li&gt;
      &lt;li&gt;Robust to viewpoint shift, object deformation, and scale transform.&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;/aircode&quot;&gt;Project Page: https://chenwang.site/aircode&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2021-10-06-aircode/object-matching1.gif&quot; /&gt;
    &lt;img src=&quot;/img/posts/2021-10-06-aircode/object-matching2.gif&quot; /&gt;
    &lt;figcaption&gt;
        A human matching demo.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Congratulations to the above young researchers!&lt;/p&gt;

&lt;p&gt;More information can be found at the &lt;a href=&quot;/research&quot;&gt;research page&lt;/a&gt;.&lt;/p&gt;</content><author><name>Chen Wang</name></author><category term="highlights" /><summary type="html">Air Series is a collection of articles mentored by Chen Wang.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://sairlab.org/img/posts/2021-12-31-air-series/AirDet-16x9.gif" /><media:content medium="image" url="https://sairlab.org/img/posts/2021-12-31-air-series/AirDet-16x9.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">First Author Publications</title><link href="https://sairlab.org/papers/" rel="alternate" type="text/html" title="First Author Publications" /><published>2021-12-28T00:00:00+00:00</published><updated>2021-12-28T00:00:00+00:00</updated><id>https://sairlab.org/first-author-paper</id><content type="html" xml:base="https://sairlab.org/papers/">&lt;p&gt;I publish papers in both robotics and computer vision.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/publications&quot;&gt;Full Publication List&lt;/a&gt;, &lt;a href=&quot;https://scholar.google.com/citations?user=vZfmKl4AAAAJ&amp;amp;hl=en&quot;&gt;Google Scholar&lt;/a&gt;, &lt;a href=&quot;https://scholars.cmu.edu/8810-chen-wang&quot;&gt;CMU Scholar&lt;/a&gt;, &lt;a href=&quot;https://dblp.org/pid/82/4206-33.html&quot;&gt;DBLP&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/a/wang_c_7.html&quot;&gt;arXiv&lt;/a&gt;, and &lt;a href=&quot;/airseries&quot;&gt;Air Series Articles&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;style&gt;
.csl-block {s
    font-size: 16px;
}
.csl-title, .csl-author, .csl-event, .csl-editor, .csl-venue {
    display: block;
    position: relative;
    font-size: 15px;
}

.csl-title b {
    font-weight: 600;
}

.csl-content {
    display: inline-block;
    vertical-align: top;
    padding-left: 20px;
}

.bibliography {
   list-style-type: none;
}
&lt;/style&gt;

&lt;ul.no-bullet class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;
    &lt;span id=&quot;wang2017non&quot;&gt;[1]&lt;div class=&quot;csl-block csl-content&quot;&gt;&lt;div class=&quot;csl-block csl-title&quot;&gt;&lt;b&gt;Non-iterative SLAM&lt;/b&gt;.&lt;/div&gt;&lt;div class=&quot;csl-block csl-author&quot;&gt;&lt;b&gt;Chen&lt;/b&gt; &lt;b&gt;Wang&lt;/b&gt;, Junsong Yuan, Lihua Xie.&lt;/div&gt;&lt;div class=&quot;csl-block csl-event&quot;&gt;&lt;i&gt;International Conference on Advanced Robotics (ICAR)&lt;/i&gt;, pp. 83–90, 2017.&lt;/div&gt;&lt;/div&gt;&lt;/span&gt;
    &lt;br /&gt;
&lt;button class=&quot;button0&quot; onclick=&quot;togglewang2017non()&quot;&gt;bibtex&lt;/button&gt;

&lt;script&gt;
    function togglewang2017non() {
        var x= document.getElementById('awang2017non');
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
&lt;/script&gt;



&lt;script&gt;
    function toggle2wang2017non() {
        var x= document.getElementById('bwang2017non');
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
&lt;/script&gt;





&lt;a href=&quot;https://arxiv.org/pdf/1701.05294&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button4&quot; value=&quot;PDF&quot; /&gt;&lt;/a&gt;



&lt;a href=&quot;https://youtu.be/Ed_6wYIKRfs&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button3&quot; value=&quot;Video&quot; /&gt;&lt;/a&gt;



&lt;b style=&quot;color:red;&quot;&gt;Best Paper Award in Robotic Planning&lt;/b&gt;

&lt;/div&gt;

&lt;div id=&quot;awang2017non&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;@inproceedings{wang2017non,
  title = {Non-iterative SLAM},
  author = {&lt;b&gt;Wang&lt;/b&gt;, &lt;b&gt;Chen&lt;/b&gt; and Yuan, Junsong and Xie, Lihua},
  booktitle = {International Conference on Advanced Robotics (ICAR)},
  pages = {83--90},
  year = {2017},
  organization = {IEEE},
  url = {https://arxiv.org/pdf/1701.05294},
  video = {https://youtu.be/Ed_6wYIKRfs},
  addendum = {Best Paper Award in Robotic Planning},
  keywords = {first}
}
&lt;/pre&gt;&lt;/div&gt;
&lt;!-- &lt;div id=&quot;bwang2017non&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;&lt;/pre&gt;&lt;/div&gt; --&gt;
&lt;/li&gt;
&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;
    &lt;span id=&quot;wang2017noniterative&quot;&gt;[2]&lt;div class=&quot;csl-block csl-content&quot;&gt;&lt;div class=&quot;csl-block csl-title&quot;&gt;&lt;b&gt;Non-iterative RGB-D-inertial Odometry&lt;/b&gt;. &lt;/div&gt;&lt;div class=&quot;csl-block csl-author&quot;&gt;&lt;b&gt;Chen&lt;/b&gt; &lt;b&gt;Wang&lt;/b&gt;, Minh-Chung Hoang, Lihua Xie, Junsong Yuan.&lt;/div&gt;&lt;div class=&quot;csl-block csl-event&quot;&gt;&lt;i&gt;arXiv preprint arXiv:1710.05502&lt;/i&gt;, 2017.&lt;/div&gt;&lt;/div&gt;&lt;/span&gt;
    &lt;br /&gt;
&lt;button class=&quot;button0&quot; onclick=&quot;togglewang2017noniterative()&quot;&gt;bibtex&lt;/button&gt;

&lt;script&gt;
    function togglewang2017noniterative() {
        var x= document.getElementById('awang2017noniterative');
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
&lt;/script&gt;



&lt;script&gt;
    function toggle2wang2017noniterative() {
        var x= document.getElementById('bwang2017noniterative');
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
&lt;/script&gt;





&lt;a href=&quot;https://arxiv.org/pdf/1710.05502&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button4&quot; value=&quot;PDF&quot; /&gt;&lt;/a&gt;



&lt;a href=&quot;https://youtu.be/zqOkHardugI&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button3&quot; value=&quot;Video&quot; /&gt;&lt;/a&gt;



&lt;/div&gt;

&lt;div id=&quot;awang2017noniterative&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;@article{wang2017noniterative,
  title = {Non-iterative RGB-D-inertial Odometry},
  author = {&lt;b&gt;Wang&lt;/b&gt;, &lt;b&gt;Chen&lt;/b&gt; and Hoang, Minh-Chung and Xie, Lihua and Yuan, Junsong},
  journal = {arXiv preprint arXiv:1710.05502},
  url = {https://arxiv.org/pdf/1710.05502},
  video = {https://youtu.be/zqOkHardugI},
  year = {2017},
  keywords = {first}
}
&lt;/pre&gt;&lt;/div&gt;
&lt;!-- &lt;div id=&quot;bwang2017noniterative&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;&lt;/pre&gt;&lt;/div&gt; --&gt;
&lt;/li&gt;
&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;
    &lt;span id=&quot;wang2017ultra&quot;&gt;[3]&lt;div class=&quot;csl-block csl-content&quot;&gt;&lt;div class=&quot;csl-block csl-title&quot;&gt;&lt;b&gt;Ultra-Wideband Aided Fast Localization and Mapping System&lt;/b&gt;.&lt;/div&gt;&lt;div class=&quot;csl-block csl-author&quot;&gt;&lt;b&gt;Chen&lt;/b&gt; &lt;b&gt;Wang&lt;/b&gt;, Handuo Zhang, Thien-Minh Nguyen, Lihua Xie.&lt;/div&gt;&lt;div class=&quot;csl-block csl-event&quot;&gt;&lt;i&gt;IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)&lt;/i&gt;, pp. 1602–1609, 2017.&lt;/div&gt;&lt;/div&gt;&lt;/span&gt;
    &lt;br /&gt;
&lt;button class=&quot;button0&quot; onclick=&quot;togglewang2017ultra()&quot;&gt;bibtex&lt;/button&gt;

&lt;script&gt;
    function togglewang2017ultra() {
        var x= document.getElementById('awang2017ultra');
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
&lt;/script&gt;



&lt;script&gt;
    function toggle2wang2017ultra() {
        var x= document.getElementById('bwang2017ultra');
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
&lt;/script&gt;





&lt;a href=&quot;https://arxiv.org/pdf/1710.00156&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button4&quot; value=&quot;PDF&quot; /&gt;&lt;/a&gt;


&lt;a href=&quot;https://github.com/wang-chen/localization&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button3&quot; value=&quot;Code&quot; /&gt;&lt;/a&gt;




&lt;/div&gt;

&lt;div id=&quot;awang2017ultra&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;@inproceedings{wang2017ultra,
  title = {Ultra-Wideband Aided Fast Localization and Mapping System},
  author = {&lt;b&gt;Wang&lt;/b&gt;, &lt;b&gt;Chen&lt;/b&gt; and Zhang, Handuo and Nguyen, Thien-Minh and Xie, Lihua},
  booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year = {2017},
  pages = {1602--1609},
  url = {https://arxiv.org/pdf/1710.00156},
  code = {https://github.com/wang-chen/localization},
  organization = {IEEE},
  keywords = {first}
}
&lt;/pre&gt;&lt;/div&gt;
&lt;!-- &lt;div id=&quot;bwang2017ultra&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;&lt;/pre&gt;&lt;/div&gt; --&gt;
&lt;/li&gt;
&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;
    &lt;span id=&quot;wang2018kernel&quot;&gt;[4]&lt;div class=&quot;csl-block csl-content&quot;&gt;&lt;div class=&quot;csl-block csl-title&quot;&gt;&lt;b&gt;Kernel Cross-Correlator&lt;/b&gt;.&lt;/div&gt;&lt;div class=&quot;csl-block csl-author&quot;&gt;&lt;b&gt;Chen&lt;/b&gt; &lt;b&gt;Wang&lt;/b&gt;, Le Zhang, Lihua Xie, Junsong Yuan.&lt;/div&gt;&lt;div class=&quot;csl-block csl-event&quot;&gt;&lt;i&gt;Thirty-Second AAAI Conference on Artificial Intelligence (AAAI)&lt;/i&gt;, pp. 4179–4186, 2018.&lt;/div&gt;&lt;/div&gt;&lt;/span&gt;
    &lt;br /&gt;
&lt;button class=&quot;button0&quot; onclick=&quot;togglewang2018kernel()&quot;&gt;bibtex&lt;/button&gt;

&lt;script&gt;
    function togglewang2018kernel() {
        var x= document.getElementById('awang2018kernel');
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
&lt;/script&gt;



&lt;script&gt;
    function toggle2wang2018kernel() {
        var x= document.getElementById('bwang2018kernel');
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
&lt;/script&gt;





&lt;a href=&quot;https://arxiv.org/pdf/1709.05936&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button4&quot; value=&quot;PDF&quot; /&gt;&lt;/a&gt;


&lt;a href=&quot;https://github.com/wang-chen/KCC&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button3&quot; value=&quot;Code&quot; /&gt;&lt;/a&gt;




&lt;/div&gt;

&lt;div id=&quot;awang2018kernel&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;@inproceedings{wang2018kernel,
  title = {Kernel Cross-Correlator},
  author = {&lt;b&gt;Wang&lt;/b&gt;, &lt;b&gt;Chen&lt;/b&gt; and Zhang, Le and Xie, Lihua and Yuan, Junsong},
  booktitle = {Thirty-Second AAAI Conference on Artificial Intelligence (AAAI)},
  pages = {4179--4186},
  year = {2018},
  url = {https://arxiv.org/pdf/1709.05936},
  code = {https://github.com/wang-chen/KCC},
  keywords = {first}
}
&lt;/pre&gt;&lt;/div&gt;
&lt;!-- &lt;div id=&quot;bwang2018kernel&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;&lt;/pre&gt;&lt;/div&gt; --&gt;
&lt;/li&gt;
&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;
    &lt;span id=&quot;wang2018correlation&quot;&gt;[5]&lt;div class=&quot;csl-block csl-content&quot;&gt;&lt;div class=&quot;csl-block csl-title&quot;&gt;&lt;b&gt;Correlation Flow: Robust Optical Flow Using Kernel Cross-Correlators&lt;/b&gt;.&lt;/div&gt;&lt;div class=&quot;csl-block csl-author&quot;&gt;&lt;b&gt;Chen&lt;/b&gt; &lt;b&gt;Wang&lt;/b&gt;, Tete Ji, Thien-Minh Nguyen, Lihua Xie.&lt;/div&gt;&lt;div class=&quot;csl-block csl-event&quot;&gt;&lt;i&gt;2018 International Conference on Robotics and Automation (ICRA)&lt;/i&gt;, pp. 836–841, 2018.&lt;/div&gt;&lt;/div&gt;&lt;/span&gt;
    &lt;br /&gt;
&lt;button class=&quot;button0&quot; onclick=&quot;togglewang2018correlation()&quot;&gt;bibtex&lt;/button&gt;

&lt;script&gt;
    function togglewang2018correlation() {
        var x= document.getElementById('awang2018correlation');
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
&lt;/script&gt;



&lt;script&gt;
    function toggle2wang2018correlation() {
        var x= document.getElementById('bwang2018correlation');
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
&lt;/script&gt;





&lt;a href=&quot;https://arxiv.org/pdf/1802.07078&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button4&quot; value=&quot;PDF&quot; /&gt;&lt;/a&gt;


&lt;a href=&quot;https://github.com/wang-chen/correlation_flow&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button3&quot; value=&quot;Code&quot; /&gt;&lt;/a&gt;




&lt;/div&gt;

&lt;div id=&quot;awang2018correlation&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;@inproceedings{wang2018correlation,
  title = {Correlation Flow: Robust Optical Flow Using Kernel Cross-Correlators},
  author = {&lt;b&gt;Wang&lt;/b&gt;, &lt;b&gt;Chen&lt;/b&gt; and Ji, Tete and Nguyen, Thien-Minh and Xie, Lihua},
  booktitle = {2018 International Conference on Robotics and Automation (ICRA)},
  pages = {836--841},
  year = {2018},
  url = {https://arxiv.org/pdf/1802.07078},
  code = {https://github.com/wang-chen/correlation_flow},
  keywords = {first}
}
&lt;/pre&gt;&lt;/div&gt;
&lt;!-- &lt;div id=&quot;bwang2018correlation&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;&lt;/pre&gt;&lt;/div&gt; --&gt;
&lt;/li&gt;
&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;
    &lt;span id=&quot;wang2019kervolutional&quot;&gt;[6]&lt;div class=&quot;csl-block csl-content&quot;&gt;&lt;div class=&quot;csl-block csl-title&quot;&gt;&lt;b&gt;Kervolutional Neural Networks&lt;/b&gt;.&lt;/div&gt;&lt;div class=&quot;csl-block csl-author&quot;&gt;&lt;b&gt;Chen&lt;/b&gt; &lt;b&gt;Wang&lt;/b&gt;, Jianfei Yang, Lihua Xie, Junsong Yuan.&lt;/div&gt;&lt;div class=&quot;csl-block csl-event&quot;&gt;&lt;i&gt;IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/i&gt;, pp. 31–40, 2019.&lt;/div&gt;&lt;/div&gt;&lt;/span&gt;
    &lt;br /&gt;
&lt;button class=&quot;button0&quot; onclick=&quot;togglewang2019kervolutional()&quot;&gt;bibtex&lt;/button&gt;

&lt;script&gt;
    function togglewang2019kervolutional() {
        var x= document.getElementById('awang2019kervolutional');
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
&lt;/script&gt;



&lt;script&gt;
    function toggle2wang2019kervolutional() {
        var x= document.getElementById('bwang2019kervolutional');
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
&lt;/script&gt;





&lt;a href=&quot;https://arxiv.org/pdf/1904.03955&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button4&quot; value=&quot;PDF&quot; /&gt;&lt;/a&gt;


&lt;a href=&quot;https://github.com/wang-chen/kervolution&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button3&quot; value=&quot;Code&quot; /&gt;&lt;/a&gt;




&lt;b style=&quot;color:red;&quot;&gt;Selected as Oral Presentation (5.6%)&lt;/b&gt;

&lt;/div&gt;

&lt;div id=&quot;awang2019kervolutional&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;@inproceedings{wang2019kervolutional,
  title = {Kervolutional Neural Networks},
  author = {&lt;b&gt;Wang&lt;/b&gt;, &lt;b&gt;Chen&lt;/b&gt; and Yang, Jianfei and Xie, Lihua and Yuan, Junsong},
  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages = {31--40},
  year = {2019},
  url = {https://arxiv.org/pdf/1904.03955},
  code = {https://github.com/wang-chen/kervolution},
  addendum = {Selected as Oral Presentation (5.6%)},
  keywords = {first}
}
&lt;/pre&gt;&lt;/div&gt;
&lt;!-- &lt;div id=&quot;bwang2019kervolutional&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;&lt;/pre&gt;&lt;/div&gt; --&gt;
&lt;/li&gt;
&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;
    &lt;span id=&quot;wang2020visual&quot;&gt;[7]&lt;div class=&quot;csl-block csl-content&quot;&gt;&lt;div class=&quot;csl-block csl-title&quot;&gt;&lt;b&gt;Visual Memorability for Robotic Interestingness via Unsupervised Online Learning&lt;/b&gt;.&lt;/div&gt;&lt;div class=&quot;csl-block csl-author&quot;&gt;&lt;b&gt;Chen&lt;/b&gt; &lt;b&gt;Wang&lt;/b&gt;, Wenshan Wang, Yuheng Qiu, Yafei Hu, Sebastian Scherer.&lt;/div&gt;&lt;div class=&quot;csl-block csl-event&quot;&gt;&lt;i&gt;European Conference on Computer Vision (ECCV)&lt;/i&gt;, pp. 52–68, 2020.&lt;/div&gt;&lt;/div&gt;&lt;/span&gt;
    &lt;br /&gt;
&lt;button class=&quot;button0&quot; onclick=&quot;togglewang2020visual()&quot;&gt;bibtex&lt;/button&gt;

&lt;script&gt;
    function togglewang2020visual() {
        var x= document.getElementById('awang2020visual');
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
&lt;/script&gt;



&lt;script&gt;
    function toggle2wang2020visual() {
        var x= document.getElementById('bwang2020visual');
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
&lt;/script&gt;





&lt;a href=&quot;https://arxiv.org/pdf/2005.08829&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button4&quot; value=&quot;PDF&quot; /&gt;&lt;/a&gt;


&lt;a href=&quot;https://github.com/wang-chen/interestingness/tree/eccv&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button3&quot; value=&quot;Code&quot; /&gt;&lt;/a&gt;


&lt;a href=&quot;https://youtu.be/PXIcm17fEko&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button3&quot; value=&quot;Video&quot; /&gt;&lt;/a&gt;



&lt;b style=&quot;color:red;&quot;&gt;Selected as Oral Presentation (2%)&lt;/b&gt;

&lt;/div&gt;

&lt;div id=&quot;awang2020visual&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;@inproceedings{wang2020visual,
  title = {Visual Memorability for Robotic Interestingness via Unsupervised Online Learning},
  author = {&lt;b&gt;Wang&lt;/b&gt;, &lt;b&gt;Chen&lt;/b&gt; and Wang, Wenshan and Qiu, Yuheng and Hu, Yafei and Scherer, Sebastian},
  booktitle = {European Conference on Computer Vision (ECCV)},
  year = {2020},
  pages = {52--68},
  url = {https://arxiv.org/pdf/2005.08829},
  code = {https://github.com/wang-chen/interestingness/tree/eccv},
  video = {https://youtu.be/PXIcm17fEko},
  addendum = {Selected as Oral Presentation (2\%)},
  keywords = {first}
}
&lt;/pre&gt;&lt;/div&gt;
&lt;!-- &lt;div id=&quot;bwang2020visual&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;&lt;/pre&gt;&lt;/div&gt; --&gt;
&lt;/li&gt;
&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;
    &lt;span id=&quot;wang2021unsupervised&quot;&gt;[8]&lt;div class=&quot;csl-block csl-content&quot;&gt;&lt;div class=&quot;csl-block csl-title&quot;&gt;&lt;b&gt;Unsupervised Online Learning for Robotic Interestingness with Visual Memory&lt;/b&gt;. &lt;/div&gt;&lt;div class=&quot;csl-block csl-author&quot;&gt;&lt;b&gt;Chen&lt;/b&gt; &lt;b&gt;Wang&lt;/b&gt;, Wenshan Wang, Yuheng Qiu, Yafei Hu, Seungchan Kim, Sebastian Scherer.&lt;/div&gt;&lt;div class=&quot;csl-block csl-event&quot;&gt;&lt;i&gt;IEEE Transactions on Robotics (TRO)&lt;/i&gt;, 2021.&lt;/div&gt;&lt;/div&gt;&lt;/span&gt;
    &lt;br /&gt;
&lt;button class=&quot;button0&quot; onclick=&quot;togglewang2021unsupervised()&quot;&gt;bibtex&lt;/button&gt;

&lt;script&gt;
    function togglewang2021unsupervised() {
        var x= document.getElementById('awang2021unsupervised');
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
&lt;/script&gt;



&lt;script&gt;
    function toggle2wang2021unsupervised() {
        var x= document.getElementById('bwang2021unsupervised');
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
&lt;/script&gt;





&lt;a href=&quot;https://arxiv.org/pdf/2111.09793&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button4&quot; value=&quot;PDF&quot; /&gt;&lt;/a&gt;


&lt;a href=&quot;https://github.com/wang-chen/interestingness&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button3&quot; value=&quot;Code&quot; /&gt;&lt;/a&gt;




&lt;/div&gt;

&lt;div id=&quot;awang2021unsupervised&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;@article{wang2021unsupervised,
  title = {Unsupervised Online Learning for Robotic Interestingness with Visual Memory},
  author = {&lt;b&gt;Wang&lt;/b&gt;, &lt;b&gt;Chen&lt;/b&gt; and Wang, Wenshan and Qiu, Yuheng and Hu, Yafei and Kim, Seungchan and Scherer, Sebastian},
  journal = {IEEE Transactions on Robotics (TRO)},
  url = {https://arxiv.org/pdf/2111.09793},
  code = {https://github.com/wang-chen/interestingness},
  year = {2021},
  keywords = {first}
}
&lt;/pre&gt;&lt;/div&gt;
&lt;!-- &lt;div id=&quot;bwang2021unsupervised&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;&lt;/pre&gt;&lt;/div&gt; --&gt;
&lt;/li&gt;
&lt;li&gt;&lt;div class=&quot;text-justify&quot;&gt;
    &lt;span id=&quot;wang2020lifelong&quot;&gt;[9]&lt;div class=&quot;csl-block csl-content&quot;&gt;&lt;div class=&quot;csl-block csl-title&quot;&gt;&lt;b&gt;Lifelong Graph Learning&lt;/b&gt;.&lt;/div&gt;&lt;div class=&quot;csl-block csl-author&quot;&gt;&lt;b&gt;Chen&lt;/b&gt; &lt;b&gt;Wang&lt;/b&gt;, Yuheng Qiu, Dasong Gao, Sebastian Scherer.&lt;/div&gt;&lt;div class=&quot;csl-block csl-event&quot;&gt;&lt;i&gt;IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)&lt;/i&gt;, 2022.&lt;/div&gt;&lt;/div&gt;&lt;/span&gt;
    &lt;br /&gt;
&lt;button class=&quot;button0&quot; onclick=&quot;togglewang2020lifelong()&quot;&gt;bibtex&lt;/button&gt;

&lt;script&gt;
    function togglewang2020lifelong() {
        var x= document.getElementById('awang2020lifelong');
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
&lt;/script&gt;



&lt;script&gt;
    function toggle2wang2020lifelong() {
        var x= document.getElementById('bwang2020lifelong');
        if (x.style.display === 'block') {
            x.style.display = 'none';
        } else {
            x.style.display = 'block';
        }
    }
&lt;/script&gt;





&lt;a href=&quot;https://arxiv.org/pdf/2009.00647&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button4&quot; value=&quot;PDF&quot; /&gt;&lt;/a&gt;


&lt;a href=&quot;https://github.com/wang-chen/LGL&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button3&quot; value=&quot;Code&quot; /&gt;&lt;/a&gt;


&lt;a href=&quot;https://youtu.be/tmAMuoi8Cs4&quot;&gt;&lt;input type=&quot;button&quot; class=&quot;button3&quot; value=&quot;Video&quot; /&gt;&lt;/a&gt;



&lt;b style=&quot;color:red;&quot;&gt;Selected as Oral Presentation (4.2%)&lt;/b&gt;

&lt;/div&gt;

&lt;div id=&quot;awang2020lifelong&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;@inproceedings{wang2020lifelong,
  title = {Lifelong Graph Learning},
  author = {&lt;b&gt;Wang&lt;/b&gt;, &lt;b&gt;Chen&lt;/b&gt; and Qiu, Yuheng and Gao, Dasong and Scherer, Sebastian},
  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2022},
  url = {https://arxiv.org/pdf/2009.00647},
  code = {https://github.com/wang-chen/LGL},
  video = {https://youtu.be/tmAMuoi8Cs4},
  addendum = {Selected as Oral Presentation (4.2%)},
  keywords = {first}
}
&lt;/pre&gt;&lt;/div&gt;
&lt;!-- &lt;div id=&quot;bwang2020lifelong&quot; style=&quot;display:none&quot;&gt;&lt;pre&gt;&lt;/pre&gt;&lt;/div&gt; --&gt;
&lt;/li&gt;&lt;/ul.no-bullet&gt;</content><author><name>Chen Wang</name></author><category term="highlights" /><summary type="html">I publish papers in both robotics and computer vision.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://sairlab.org/img/posts/highlights/interestingness.png" /><media:content medium="image" url="https://sairlab.org/img/posts/highlights/interestingness.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">AirCode: A Robust Object Encoding Method</title><link href="https://sairlab.org/aircode/" rel="alternate" type="text/html" title="AirCode: A Robust Object Encoding Method" /><published>2021-10-06T12:00:00+00:00</published><updated>2021-10-06T12:00:00+00:00</updated><id>https://sairlab.org/aircode</id><content type="html" xml:base="https://sairlab.org/aircode/">&lt;p&gt;Object encoding and identification is crucial for many robotic tasks such as autonomous exploration and semantic relocalization. Existing works heavily rely on the tracking of detected objects but have difficulty to recall revisited objects precisely. In this work, we propose a novel object encoding method, &lt;strong&gt;AirCode&lt;/strong&gt;, based on a graph of key-points. To be robust to the number of key-points detected, we propose a feature sparse encoding and object dense encoding method to ensure that each key-point can only affect a small part of the object descriptors, leading it to be robust to viewpoint changes, scaling, occlusion, and even object deformation.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2021-10-06-aircode/object-matching1.gif&quot; /&gt;
    &lt;img src=&quot;/img/posts/2021-10-06-aircode/object-matching2.gif&quot; /&gt;
    &lt;figcaption&gt;
        A human matching demo.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;object-representation&quot;&gt;Object Representation&lt;/h2&gt;

&lt;p&gt;To save computational resources, object matching in SLAM is often based on key-point features, as the feature-based SLAM methods are still widely used. Inspired by the recent progresses in deep learning-based key-point detector and feature matching methods, it becomes intuitive to encode an object via a group of key-points in an end-to-end manner, where the key-points on the same object form a graph neural network. Therefore, we can take the graph embeddings as the object descriptors.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2021-10-06-aircode/demo.jpg&quot; /&gt;
    &lt;figcaption&gt;
        Four objects including 1 rigid object (laptop) and 3 non-rigid objects (human) are identified. AirCode is insensitive to viewpoint (Obj 1 in (b) and (d)), scaling (Obj 3 and Obj 4), occlusion (Obj 4), and even posture change (Obj 2 and Obj 3).
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;During robot exploration, robots often observe part of the objects due to occlusion and different viewpoints, resulting in that the object key-points only have a small overlap across different frames. Therefore, the key-points graph embedding will be easily affected, which makes it difficult to directly apply a graph network. To solve this problem, we argue that a key-point descriptor should have a sparse effect on the object embedding. This means that only a few positions of an object descriptor can be affected if a key-point is added or removed from an object graph. To achieve this, we propose a sparse object encoding method, which is robust to the change of viewpoint and object deformation.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2021-10-06-aircode/kitti-relocalization.gif&quot; /&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;video&quot;&gt;Video&lt;/h2&gt;

&lt;div class=&quot;video-wrapper&quot;&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/ZhW4Qk1tLNQ&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;h2 id=&quot;publication&quot;&gt;Publication&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Kuan Xu, Chen Wang, Chao Chen, Wei Wu, Sebastian Scherer. AirCode: A Robust Object Encoding Method.” &lt;em&gt;IEEE Robotics and Automation Letters (RA-L)&lt;/em&gt;, 2022. (Accepted to ICRA 2022)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Please refer to our &lt;a href=&quot;https://arxiv.org/pdf/2105.00327&quot;&gt;Paper&lt;/a&gt; and &lt;a href=&quot;https://github.com/wang-chen/AirCode&quot;&gt;Code&lt;/a&gt; for details.&lt;/p&gt;

&lt;h2 id=&quot;contact&quot;&gt;Contact&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://chenwang.site&quot;&gt;Chen Wang&lt;/a&gt; &amp;lt;chenwang [at] dr.com&amp;gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://theairlab.org/team/sebastian/&quot;&gt;Sebastian Scherer&lt;/a&gt; &amp;lt;basti [at] cmu [dot] edu&amp;gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Chen Wang</name></author><category term="research" /><summary type="html">Object encoding and identification is crucial for many robotic tasks such as autonomous exploration and semantic relocalization. Existing works heavily rely on the tracking of detected objects but have difficulty to recall revisited objects precisely. In this work, we propose a novel object encoding method, AirCode, based on a graph of key-points. To be robust to the number of key-points detected, we propose a feature sparse encoding and object dense encoding method to ensure that each key-point can only affect a small part of the object descriptors, leading it to be robust to viewpoint changes, scaling, occlusion, and even object deformation.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://sairlab.org/img/posts/2021-10-06-aircode/object-matching1.gif" /><media:content medium="image" url="https://sairlab.org/img/posts/2021-10-06-aircode/object-matching1.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">AirLoop: Lifelong loop closure detection</title><link href="https://sairlab.org/airloop/" rel="alternate" type="text/html" title="AirLoop: Lifelong loop closure detection" /><published>2021-09-28T02:00:07+00:00</published><updated>2021-09-28T02:00:07+00:00</updated><id>https://sairlab.org/airloop</id><content type="html" xml:base="https://sairlab.org/airloop/">&lt;p&gt;After deployment to the wild, the robot may benefit if it can incrementally learn from the working environments with newly observed data. 
However, as the perception algorithm involves more and more deep networks for improved robustness over traditional methods, this becomes hard: Deep models typically need to adjust all parameters jointly to adapt to new environments.&lt;/p&gt;

&lt;p&gt;This poses a dilemma: On the one hand, if we only retrain on the data increments, fitting to the new data almost inevitably alters the parameters and, subsequently, the representations learned from the old data, leading to performance degradation over time.
This phenomenon is referred to as catastrophic forgetting.
On the other hand, if all previously observed data is retained to perform joint training after observing each new environment, the storage cost will grow linearly as the model learns from one environment to the next.
This is prohibitive on resource-constrained devices such as a drone’s onboard computer.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2021-09-28-airloop/motivation.gif&quot; /&gt;
    &lt;figcaption&gt;
        A robot may encounter a series of new environments after deployment from which it can gradually lean a more suitable scene representation.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;lifelong-loop-closure-detection&quot;&gt;Lifelong loop closure detection&lt;/h2&gt;

&lt;p&gt;In this work, we focus on the specific problem of learning a loop closure detection (LCD) network in an incremental fashion.
We developed &lt;strong&gt;AirLoop&lt;/strong&gt;, a method that leverages techniques from lifelong learning to minimize forgetting when training loop closure detection models incrementally.
To the best of our knowledge, AirLoop is one of the first work to study LCD in the lifelong learning context.&lt;/p&gt;

&lt;p&gt;To achieve lifelong learning of LCD models, it is important to ensure that the geometry of the global descriptor space (i.e. descriptors’ pairwise similarities) does not deform as the network trains in more environments.
For this purpose, we adopted the relational variant of two regularization losses for lifelong classification, dubbed RMAS (relational memory aware synapses) and RKD (relational knowledge distillation).
The former operates by calculating the descriptor space’s “sensitivity” with respect to each parameter and use it to selectively penalize future changes to the parameters, whereas the later compare the output of the current network against its previous versions and protects the descriptor space from uncontrolled deformation.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2021-09-28-airloop/approach.gif&quot; /&gt;
    &lt;figcaption&gt;
        Lifelong losses in AirLoop. RMAS and RKD regularizes the network in parameter space and embedding space, respectively.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Both method combined allows us to train the LCD model in a series of environments sequentially without suffering from serious catastrophic forgetting while only requiring constant memory and computation.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;Here we show the lifelong learning outcome of the model on &lt;a href=&quot;https://theairlab.org/tartanair-dataset/&quot;&gt;TartanAir&lt;/a&gt;, &lt;a href=&quot;https://webdiis.unizar.es/~jmfacil/pr-nordland/&quot;&gt;Nordland&lt;/a&gt;, and &lt;a href=&quot;https://robotcar-dataset.robots.ox.ac.uk/&quot;&gt;RobotCar&lt;/a&gt;:&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2021-09-28-airloop/all-datasets.png&quot; /&gt;
    &lt;figcaption&gt;
        Examples of loop closure detection on each dataset. Note that our model is able to handle cross-environment loop closure detection despite only trained in individual environments sequentially.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;We also observe improved robustness after extended training:&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/2021-09-28-airloop/tartanair-ll.gif&quot; /&gt;
    &lt;figcaption&gt;
        Model performance comparison on TartanAir. The model is able to correct some of the previously made mistakes after training in more environments.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;video&quot;&gt;Video&lt;/h2&gt;

&lt;div class=&quot;video-wrapper&quot;&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/Gr9i5ONNmz0&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;h2 id=&quot;publication&quot;&gt;Publication&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Dasong Gao, Chen Wang, and Sebastian Scherer. AirLoop: Lifelong Loop Closure Detection.” &lt;em&gt;2022 International Conference on Robotics and Automation (ICRA)&lt;/em&gt;, 2022.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This paper in submission to ICRA 2022.
Please refer to our &lt;a href=&quot;https://arxiv.org/pdf/2109.08975&quot;&gt;paper&lt;/a&gt; and &lt;a href=&quot;https://github.com/wang-chen/AirLoop&quot;&gt;code&lt;/a&gt; for details.&lt;/p&gt;

&lt;h2 id=&quot;contact&quot;&gt;Contact&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://theairlab.org/team/dasongg/&quot;&gt;Dasong Gao&lt;/a&gt; &amp;lt;dasongg [at] andrew [dot] cmu [dot] edu&amp;gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://chenwang.site&quot;&gt;Chen Wang&lt;/a&gt; &amp;lt;chenwang [at] dr.com&amp;gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://theairlab.org/team/sebastian/&quot;&gt;Sebastian Scherer&lt;/a&gt; &amp;lt;basti [at] cmu [dot] edu&amp;gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Dasong Gao</name></author><category term="research" /><summary type="html">After deployment to the wild, the robot may benefit if it can incrementally learn from the working environments with newly observed data. However, as the perception algorithm involves more and more deep networks for improved robustness over traditional methods, this becomes hard: Deep models typically need to adjust all parameters jointly to adapt to new environments. This poses a dilemma: On the one hand, if we only retrain on the data increments, fitting to the new data almost inevitably alters the parameters and, subsequently, the representations learned from the old data, leading to performance degradation over time. This phenomenon is referred to as catastrophic forgetting. On the other hand, if all previously observed data is retained to perform joint training after observing each new environment, the storage cost will grow linearly as the model learns from one environment to the next. This is prohibitive on resource-constrained devices such as a drone’s onboard computer. A robot may encounter a series of new environments after deployment from which it can gradually lean a more suitable scene representation. Lifelong loop closure detection In this work, we focus on the specific problem of learning a loop closure detection (LCD) network in an incremental fashion. We developed AirLoop, a method that leverages techniques from lifelong learning to minimize forgetting when training loop closure detection models incrementally. To the best of our knowledge, AirLoop is one of the first work to study LCD in the lifelong learning context. To achieve lifelong learning of LCD models, it is important to ensure that the geometry of the global descriptor space (i.e. descriptors’ pairwise similarities) does not deform as the network trains in more environments. For this purpose, we adopted the relational variant of two regularization losses for lifelong classification, dubbed RMAS (relational memory aware synapses) and RKD (relational knowledge distillation). The former operates by calculating the descriptor space’s “sensitivity” with respect to each parameter and use it to selectively penalize future changes to the parameters, whereas the later compare the output of the current network against its previous versions and protects the descriptor space from uncontrolled deformation. Lifelong losses in AirLoop. RMAS and RKD regularizes the network in parameter space and embedding space, respectively. Both method combined allows us to train the LCD model in a series of environments sequentially without suffering from serious catastrophic forgetting while only requiring constant memory and computation. Results Here we show the lifelong learning outcome of the model on TartanAir, Nordland, and RobotCar: Examples of loop closure detection on each dataset. Note that our model is able to handle cross-environment loop closure detection despite only trained in individual environments sequentially. We also observe improved robustness after extended training: Model performance comparison on TartanAir. The model is able to correct some of the previously made mistakes after training in more environments. Video Publication Dasong Gao, Chen Wang, and Sebastian Scherer. AirLoop: Lifelong Loop Closure Detection.” 2022 International Conference on Robotics and Automation (ICRA), 2022. This paper in submission to ICRA 2022. Please refer to our paper and code for details. Contact Dasong Gao &amp;lt;dasongg [at] andrew [dot] cmu [dot] edu&amp;gt; Chen Wang &amp;lt;chenwang [at] dr.com&amp;gt; Sebastian Scherer &amp;lt;basti [at] cmu [dot] edu&amp;gt;</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://sairlab.org/img/posts/2021-09-28-airloop/cover.png" /><media:content medium="image" url="https://sairlab.org/img/posts/2021-09-28-airloop/cover.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">F-LOAM: Fast LiDAR Odometry and Mapping</title><link href="https://sairlab.org/floam/" rel="alternate" type="text/html" title="F-LOAM: Fast LiDAR Odometry and Mapping" /><published>2021-06-03T00:00:00+00:00</published><updated>2021-06-03T00:00:00+00:00</updated><id>https://sairlab.org/floam</id><content type="html" xml:base="https://sairlab.org/floam/">&lt;p&gt;FLOAM was included in the &lt;a href=&quot;https://discourse.ros.org/t/new-packages-for-noetic-2021-09-08/22213&quot;&gt;ROS Noetic update&lt;/a&gt;, which is now available!&lt;/p&gt;

&lt;p&gt;Simultaneous Localization and Mapping (SLAM) has wide robotic applications such as autonomous driving
and unmanned aerial vehicles. Both computational efficiency and localization accuracy are of great importance to a good SLAM system.
Existing works on LiDAR based SLAM often formulate the problem as two modules: scan-to-scan match and scan-to-map refinement, which are often solved by iterative calculation which are computationally expensive.
In this work, we propose a general solution that aims to provide a computationally efficient and accurate framework for LiDAR based SLAM.&lt;/p&gt;

&lt;figure&gt;
 &lt;img src=&quot;/img/posts/2021-09-18-floam/floam_mapping.gif&quot; alt=&quot;FLOAM Mapping&quot; /&gt;
 &lt;figcaption&gt;
    Mapping with F-LOAM
 &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;We adopt a non-iterative two-stage distortion compensation method to reduce the computational cost.
For each scan input, the edge and planar features are extracted and matched to a local edge map and a local plane map separately, where the local smoothness is also considered for iterative pose optimization.&lt;/p&gt;

&lt;!-- Thorough experiments are performed to evaluate its performance in challenging scenarios, including localization for a warehouse Automated Guided Vehicle (AGV) and a public dataset on autonomous driving.  --&gt;

&lt;figure&gt;
 &lt;img src=&quot;/img/posts/2021-09-18-floam/kitti_example.gif&quot; alt=&quot;FLOAM Localization&quot; /&gt;
 &lt;figcaption&gt;
    An example of Localization with FLOAM
 &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;F-LOAM achieves a competitive localization accuracy with a processing rate of more than 10 Hz in the public dataset evaluation, which provides a good trade-off between performance and computational cost for practical applications.
It is one of the most accurate and &lt;a href=&quot;https://github.com/wh200720041/floam&quot;&gt;fastest open-sourced SLAM systems&lt;/a&gt; in KITTI dataset ranking.&lt;/p&gt;

&lt;h2 id=&quot;performance&quot;&gt;Performance&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Computational efficiency evaluation (based on KITTI dataset):&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Dataset&lt;/th&gt;
      &lt;th&gt;ALOAM&lt;/th&gt;
      &lt;th&gt;FLOAM&lt;/th&gt;
      &lt;th&gt;Platform&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;KITTI&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;151ms&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;59ms&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;Intel® Core™ i7-8700 CPU @ 3.20GHz&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;Localization error:&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Dataset&lt;/th&gt;
      &lt;th&gt;ALOAM&lt;/th&gt;
      &lt;th&gt;FLOAM&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;KITTI sequence 00&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;0.55%&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;0.51%&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;KITTI sequence 02&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;3.93%&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;1.25%&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;KITTI sequence 05&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;1.28%&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;0.93%&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;video-wrapper&quot;&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/QvXN5XhAYYw&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;h3 id=&quot;project-members&quot;&gt;Project Members&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://wanghan.pro&quot;&gt;Han Wang&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://chenwang.site&quot;&gt;Chen Wang&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://scholar.google.com.sg/citations?user=Fmrv3J8AAAAJ&amp;amp;hl=en&quot;&gt;Lihua Xie&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;publication&quot;&gt;Publication&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Han Wang, Chen Wang, Lihua Xie, “F-LOAM: Fast LiDAR Odometry And Mapping”. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021. [&lt;a href=&quot;https://arxiv.org/abs/2107.00822&quot;&gt;PDF&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;source-codes&quot;&gt;Source Codes&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ROS Package: &lt;a href=&quot;https://github.com/wh200720041/floam&quot;&gt;floam&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;FLOAM was included in &lt;a href=&quot;https://discourse.ros.org/t/new-packages-for-noetic-2021-09-08/22213&quot;&gt;ROS Noetic&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;citation&quot;&gt;Citation&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@inproceedings{wang2021f,
 title = {F-LOAM: Fast LiDAR Odometry And Mapping},
 author = {Wang, Han and Wang, Chen and Chen, Chun-Lin and Xie, Lihua},
 booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 year = {2021},
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Han Wang</name></author><category term="research" /><summary type="html">FLOAM was included in the ROS Noetic update, which is now available!</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://sairlab.org/img/posts/2021-09-18-floam/floam_kitti.gif" /><media:content medium="image" url="https://sairlab.org/img/posts/2021-09-18-floam/floam_kitti.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Unsupervised Online Learning for Visual Interestingness</title><link href="https://sairlab.org/interestingness/" rel="alternate" type="text/html" title="Unsupervised Online Learning for Visual Interestingness" /><published>2020-05-01T10:50:07+00:00</published><updated>2020-05-01T10:50:07+00:00</updated><id>https://sairlab.org/interestingness</id><content type="html" xml:base="https://sairlab.org/interestingness/">&lt;hr /&gt;

&lt;p&gt;The problem of visual interestingness detection, which is crucial for many practical applications such as search and rescue, is explored in this project. Although prior research is able to detect significant objects or scenes, it is not able to adapt in real-time and loose interest over time after repeatedly observing the same objects or exploring the same scenes. To enable such behaviours for robots, we argue that a learning system should have both life-time human-like experience learned from a large amount of unlabeled data and a short-term learning capability for limited negative labeled data. This is because robots normally only know uninteresting objects before a mission and have to change their interests during a mission. To this end, we introduce an unsupervised learning model with a memory mechanism, which is able to train in real-time without back-propagation, resulting in a much faster learning speed. Our experiments show that, although implemented on a single machine, our approach is still able to learn online and find meaningful objects for a practical search task in mine tunnels.&lt;/p&gt;

&lt;div class=&quot;video-wrapper&quot;&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/PXIcm17fEko&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;h3 id=&quot;approaches&quot;&gt;Approaches&lt;/h3&gt;
&lt;p&gt;We propose to establish an &lt;strong&gt;online learning&lt;/strong&gt; scheme to search for interesting scenes for robot exploration. On the other hand, existing algorithms are heavily dependent on back-propagation algorithm for learning, which is very computationally expensive. To solve this problem, we introduce a novel translation-invariant 4-D visual memory to identify and recall visually interesting scenes. Human beings have a great capacity to direct visual attention and judge the interestingness of a scene.&lt;/p&gt;

&lt;p&gt;For mobile robots, we find the following properties are necessary to establish a sense of visual interestingness:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Unsupervised&lt;/strong&gt;: Visual interestingness is a psychological process. Its definition is subjective and can change according to one’s experience and environments, thus labels are difficult to obtain. However, prior research mainly focuses on supervised methods, and their performance suffers in a prior unseen environment. We hypothesize that a sense of interestingness can be established for autonomous robots in an unsupervised manner.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Task-dependent&lt;/strong&gt;: In many tasks, we might only know uninteresting objects before the task is started. For example, in a mine rescue search task, the deployment will be more efficient and easier, if the robots can be taught what is not interesting in the specific scene within several minutes. In this sense, we argue that a visual interestingness detection system should be able to learn negative labeled samples quickly, thus an incremental learning method is necessary. Note that we expect the model is capable of learning negative samples, but it is not necessary in all tasks.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Therefore, to construct a practical interestingness detection system and achieve the above properties, we introduce an unsupervised online learning model with a novel memory mechanism and expect the following outcomes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Long-term learning&lt;/strong&gt;: In this stage, we expect a model to be trained off-line on a large amount of data in an unsupervised manner as human beings acquire common knowledge from experience. We also expect the training time on a single machine to be no more than the order of days.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Short-term learning&lt;/strong&gt;: For task-dependent knowledge, the model then should be able to learn from hundreds of uninteresting images in minutes. This can be done before a mission starts and is beneficial to quick robot deployment.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Online learning&lt;/strong&gt;: During mission execution the system should express the top interests in real-time and the detected interests should be lost online when they appear frequently, regardless if they exist in the uninteresting images or not. Another important aspect for online learning is no data leakage, i.e., each frame is proceeding without using information from its subsequent frames.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
 &lt;img src=&quot;/img/posts/2020-05-01-interestingness/image3crop.png&quot; alt=&quot;interesting images&quot; /&gt;
 &lt;figcaption&gt;
    Detected interesting scenes
 &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;key-results&quot;&gt;Key Results&lt;/h3&gt;
&lt;p&gt;In the &lt;a href=&quot;https://www.darpa.mil/program/darpa-subterranean-challenge&quot;&gt;DARPA Subterranean (SubT) Challenge&lt;/a&gt;, each team deploys multiple robots into several mine tunnels (GPS and wireless communication denied) to search for objects. The tunnels have a cumulative linear distance in the range of 4-8 km. The &lt;a href=&quot;http://theairlab.org/dataset/interestingness&quot;&gt;SubT front camera (SubTF) dataset&lt;/a&gt; contains seven long videos (1h) recorded by two fully autonomous unmanned ground vehicles (UGV) during their complete exploration in two tunnels during the tunnel circuit. Some of the video shots are presented in Figure 1. It can be seen that the SubTF dataset is very challenging, as the human annotation varies a lot, i.e. only 3.6% of the frames are labeled as interesting by at least 2 subjects, although 15% of the frames are labeled by at least 1 subject (Interest-1).&lt;/p&gt;

&lt;figure&gt;
 &lt;img src=&quot;/img/posts/2020-05-01-interestingness/image2.png&quot; alt=&quot;interestingness score&quot; /&gt;
 &lt;figcaption&gt;
    Fig. 1. This figure shows several examples of both uninteresting and interesting scenes in &lt;a href=&quot;http://theairlab.org/dataset/interestingness&quot;&gt;SubTF dataset&lt;/a&gt; taken by the Team Explorer who won the first place in DARPA SubT Challenge tunnel circuit. The height of green strip located at the right of each image indicates the interestingness level predicted by our unsupervised online learning algorithm when it sees the scene for the first time.
 &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Compared to human indicated interestingness the algorithm achieves an average 20% higher accuracy than the approach without online learning. The results indicate that our three-stage architecture of long-term, short-term, and online learning shows promise in representing interestingness for robots.&lt;/p&gt;

&lt;figure&gt;
 &lt;img src=&quot;/img/posts/2020-05-01-interestingness/interestingness-map.png&quot; alt=&quot;map&quot; style=&quot;width:80%&quot; /&gt;
 &lt;figcaption&gt;
    The map created by Lidar during fully autonomous exploration
 &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;project-members&quot;&gt;Project Members&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://chenwang.site&quot;&gt;Chen Wang&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://theairlab.org/team/yuheng/&quot;&gt;Yuheng Qiu&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.wangwenshan.com/&quot;&gt;Wenshan Wang&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://theairlab.org/team/yafeih/&quot;&gt;Yafei Hu&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://theairlab.org/team/seungchan_kim/&quot;&gt;Seungchan Kim&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://theairlab.org/team/sebastian/&quot;&gt;Sebastian Scherer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;publication&quot;&gt;Publication&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;Chen Wang&lt;/b&gt;, Wenshan Wang, Yuheng Qiu, Yafei Hu, Sebastian Scherer, “Visual Memorability for Robotic Interestingness Prediction via Unsupervised Online Learning”. European Conference on Computer Vision (ECCV), 2020. [&lt;a href=&quot;https://arxiv.org/abs/2005.08829&quot;&gt;PDF&lt;/a&gt;]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;b&gt;Chen Wang&lt;/b&gt;, Yuheng Qiu, Wenshan Wang, Yafei Hu, Seungchan Kim, Sebastian Scherer, “Unsupervised Online Learning for Robotic Interestingness with Visual Memory”. IEEE Transactions on Robotics (T-RO), 2021. [&lt;a href=&quot;https://arxiv.org/abs/2111.09793&quot;&gt;PDF&lt;/a&gt;]&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;source-codes&quot;&gt;Source Codes&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Plain Python Package: &lt;a href=&quot;https://github.com/wang-chen/interestingness&quot;&gt;interestingness&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/wang-chen/interestingness/tree/eccv&quot;&gt;TRO Branch&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/wang-chen/interestingness/tree/tro&quot;&gt;ECCV Branch&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ROS Package: &lt;a href=&quot;https://github.com/wang-chen/interestingness_ros&quot;&gt;interestingness_ros&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;citation&quot;&gt;Citation&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@inproceedings{wang2020visual,
 title={Visual Memorability for Robotic Interestingness via Unsupervised Online Learning},
 author={Wang, Chen and Wang, Wenshan and Qiu, Yuheng and Hu, Yafei and Scherer, Sebastian},
 booktitle={European Conference on Computer Vision (ECCV)},
 year={2020}
}

@article{wang2021unsupervised,
 title={Unsupervised Online Learning for Robotic Interestingness with Visual Memory},
 author={Wang, Chen and Qiu, Yuheng and Wang, Wenshan and Hu, Yafei and Kim, Seungchan and Scherer, Sebastian},
 journal={IEEE Transactions on Robotics (T-RO)},
 year={2021}
} 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Chen Wang</name></author><category term="research" /><summary type="html">The problem of visual interestingness detection, which is crucial for many practical applications such as search and rescue, is explored in this project. Although prior research is able to detect significant objects or scenes, it is not able to adapt in real-time and loose interest over time after repeatedly observing the same objects or exploring the same scenes. To enable such behaviours for robots, we argue that a learning system should have both life-time human-like experience learned from a large amount of unlabeled data and a short-term learning capability for limited negative labeled data. This is because robots normally only know uninteresting objects before a mission and have to change their interests during a mission. To this end, we introduce an unsupervised learning model with a memory mechanism, which is able to train in real-time without back-propagation, resulting in a much faster learning speed. Our experiments show that, although implemented on a single machine, our approach is still able to learn online and find meaningful objects for a practical search task in mine tunnels.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://sairlab.org/img/posts/2020-05-01-interestingness/interestingness.gif" /><media:content medium="image" url="https://sairlab.org/img/posts/2020-05-01-interestingness/interestingness.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Best Paper Award</title><link href="https://sairlab.org/best-icar/" rel="alternate" type="text/html" title="Best Paper Award" /><published>2020-04-20T00:00:00+00:00</published><updated>2020-04-20T00:00:00+00:00</updated><id>https://sairlab.org/best-paper-award</id><content type="html" xml:base="https://sairlab.org/best-icar/">&lt;p&gt;Paper titled “Non-iterative SLAM” received the best paper award in robotic planning from the International Conference on Advanced Robotics (ICAR).&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/highlights/ni-slam-laptop.gif&quot; /&gt;
    &lt;figcaption&gt;
        Real-time dense mapping on a laptop.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Existing SLAM methods often require iterative solutions to find data association.
For example, they require gradient descent algorithm to perform bundle adjustment, require RANSAC and ICP to match two point clouds.
However, all of them are computational expensive or sensitive to initialization.&lt;/p&gt;

&lt;p&gt;To solve this problem, we develop &lt;strong&gt;Non-iterative SLAM&lt;/strong&gt;, which has a non-iterative solution.
It is the first RGB-D-Inertial SLAM method that has a closed-from solution only with a complexity of O(N log N).&lt;/p&gt;

&lt;p&gt;Because of this, it is very computationally efficient. We can even perform real-time dense mapping on a credit card sized computing board.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/highlights/ni-slam-real-time.gif&quot; /&gt;
    &lt;img src=&quot;/img/posts/highlights/credit-card-board.png&quot; /&gt;
    &lt;figcaption&gt;
        Live demo on a credit card sized computing board. No post-processing required.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;To the best of our knowledge, it was the world’s first real-time dense mapping demo on such a small computer.
It can provide centimeter level accuracy with an onboard low power processor on a flying robot.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/highlights/ni-slam.gif&quot; /&gt;
    &lt;figcaption&gt;
        Real-time trajectory estimation on a flying robot.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The closed-form solution is based on our &lt;strong&gt;kernel cross-correlator&lt;/strong&gt; (KCC), which is published in AAAI 2017.
KCC is robust to noises, thus the trajectory estimation is also smoother than other methods.&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;/img/posts/highlights/ground-robot.png&quot; width=&quot;40%&quot; /&gt; 
    &lt;img src=&quot;/img/posts/highlights/ni-slam-ground.gif&quot; width=&quot;55%&quot; /&gt;
    &lt;figcaption&gt;
        Robot Localization only with Ground Textures.&lt;br /&gt;
        The QR code is for calculating the drift error instead of localization.
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;We also applied it to warehouse robots for high-precision localization only with ground textures.
In the tests, it only produces 0.1% to 0.5% drift error if no loop closure is given.&lt;/p&gt;

&lt;p&gt;Theoretically, the closed-form solution is based on our paper titled “Kernel Cross-Correlator”, which has a closed-form solution in frequency domain. For more detailed information, please refer to our papers.&lt;/p&gt;

&lt;div class=&quot;video-wrapper&quot;&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/Ed_6wYIKRfs&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;h3 id=&quot;related-papers&quot;&gt;Related Papers&lt;/h3&gt;

&lt;p&gt;Paper: &lt;a href=&quot;https://arxiv.org/pdf/1701.05294&quot;&gt;Non-iterative SLAM&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=&quot;https://arxiv.org/abs/1710.05502&quot;&gt;Non-iterative RGB-D-inertial Odometry&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Paper: &lt;a href=&quot;https://arxiv.org/abs/1709.05936&quot;&gt;Kernel Cross-Correlator&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;citation&quot;&gt;Citation&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@inproceedings{wang2017non,
 title={Non-iterative SLAM},
 author={Wang, Chen and Yuan, Junsong and Xie, Lihua},
 booktitle={International Conference on Advanced Robotics (ICAR)},
 pages={83--90},
 year={2017},
 organization={IEEE},
}

@article{wang2017noniterative,
 title={Non-iterative RGB-D-inertial Odometry},
 author={Wang, Chen and Hoang, Minh-Chung and Xie, Lihua and Yuan, Junsong},
 journal={arXiv preprint arXiv:1710.05502},
 year={2017},
}

@inproceedings{wang2018kernel,
 title={Kernel Cross-Correlator},
 author={Wang, Chen and Zhang, Le and Xie, Lihua and Yuan, Junsong},
 booktitle={Thirty-Second AAAI Conference on Artificial Intelligence (AAAI)},
 pages={4179--4186},
 year={2018},
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Chen Wang</name></author><category term="highlights" /><summary type="html">Paper titled “Non-iterative SLAM” received the best paper award in robotic planning from the International Conference on Advanced Robotics (ICAR).</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://sairlab.org/img/posts/highlights/ni-slam.gif" /><media:content medium="image" url="https://sairlab.org/img/posts/highlights/ni-slam.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>